{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "set_seed(42)\n",
    "print(\"Seed set to 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models and Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the tokenizers using the ```from_pretrained()``` method and print the sizes of the vocabulary for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Tokenizer Vocabulary Size: 32011\n",
      "Gemma Tokenizer Vocabulary Size: 256000\n"
     ]
    }
   ],
   "source": [
    "print(\"Phi Tokenizer Vocabulary Size:\", len(phi_tokenizer))\n",
    "print(\"Gemma Tokenizer Vocabulary Size:\", len(gemma_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose three example sentences of your choice. Tokenize each of them using the tokenizers of each model. Print the token IDs, attention masks, and the corresponding string tokens for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Germany is located in the middle of Europe.',\n",
    "    'Saarland shares a border with France and is known for its strong Franco-German ties.',\n",
    "    'CISPA focuses on information security research and is based in Saarbrücken, Saarland.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Tokenizer Outputs:\n",
      "\n",
      "Sentence: Germany is located in the middle of Europe.\n",
      "Token IDs: [9556, 338, 5982, 297, 278, 7256, 310, 4092, 29889]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['▁Germany', '▁is', '▁located', '▁in', '▁the', '▁middle', '▁of', '▁Europe', '.']\n",
      "\n",
      "Sentence: Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "Token IDs: [317, 4025, 1049, 29358, 263, 5139, 411, 3444, 322, 338, 2998, 363, 967, 4549, 20923, 29899, 29954, 3504, 260, 583, 29889]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['▁S', 'aar', 'land', '▁shares', '▁a', '▁border', '▁with', '▁France', '▁and', '▁is', '▁known', '▁for', '▁its', '▁strong', '▁Franco', '-', 'G', 'erman', '▁t', 'ies', '.']\n",
      "\n",
      "Sentence: CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n",
      "Token IDs: [315, 3235, 7228, 8569, 267, 373, 2472, 6993, 5925, 322, 338, 2729, 297, 317, 4025, 1182, 23075, 29892, 317, 4025, 1049, 29889]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['▁C', 'IS', 'PA', '▁focus', 'es', '▁on', '▁information', '▁security', '▁research', '▁and', '▁is', '▁based', '▁in', '▁S', 'aar', 'br', 'ücken', ',', '▁S', 'aar', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Phi Tokenizer Outputs:\")\n",
    "phi_tokens = []\n",
    "for sentence in sentences:\n",
    "    encoded = phi_tokenizer(sentence, return_tensors='pt')\n",
    "    token_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    tokens = phi_tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(\"Token IDs:\", token_ids[0].tolist())\n",
    "    print(\"Attention Mask:\", attention_mask[0].tolist())\n",
    "    print(\"Tokens:\", tokens)\n",
    "    phi_tokens.append(token_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Tokenizer Outputs:\n",
      "\n",
      "Sentence: Germany is located in the middle of Europe.\n",
      "Token IDs: [2, 30988, 603, 7023, 575, 573, 7185, 576, 4238, 235265]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['<bos>', 'Germany', '▁is', '▁located', '▁in', '▁the', '▁middle', '▁of', '▁Europe', '.']\n",
      "\n",
      "Sentence: Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "Token IDs: [2, 7595, 486, 1445, 14324, 476, 9994, 675, 6081, 578, 603, 3836, 604, 1277, 3779, 35961, 235290, 36419, 23572, 235265]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['<bos>', 'Sa', 'ar', 'land', '▁shares', '▁a', '▁border', '▁with', '▁France', '▁and', '▁is', '▁known', '▁for', '▁its', '▁strong', '▁Franco', '-', 'German', '▁ties', '.']\n",
      "\n",
      "Sentence: CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n",
      "Token IDs: [2, 93336, 4840, 31381, 611, 2113, 6206, 3679, 578, 603, 3482, 575, 96808, 164937, 235269, 96808, 1445, 235265]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['<bos>', 'CIS', 'PA', '▁focuses', '▁on', '▁information', '▁security', '▁research', '▁and', '▁is', '▁based', '▁in', '▁Saar', 'brücken', ',', '▁Saar', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Gemma Tokenizer Outputs:\")\n",
    "gemma_tokens = []\n",
    "for sentence in sentences:\n",
    "    encoded = gemma_tokenizer(sentence, return_tensors='pt')\n",
    "    token_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    tokens = gemma_tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(\"Token IDs:\", token_ids[0].tolist())\n",
    "    print(\"Attention Mask:\", attention_mask[0].tolist())\n",
    "    print(\"Tokens:\", tokens)\n",
    "    gemma_tokens.append(token_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now tokenize all three sentences together as a batch. Use padding. Print the token IDs, attention masks, string tokens, and the maximum sequence length after padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Tokenizer Batch Outputs:\n",
      "Token IDs: tensor([[32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "         32000, 32000, 32000,  9556,   338,  5982,   297,   278,  7256,   310,\n",
      "          4092, 29889],\n",
      "        [32000,   317,  4025,  1049, 29358,   263,  5139,   411,  3444,   322,\n",
      "           338,  2998,   363,   967,  4549, 20923, 29899, 29954,  3504,   260,\n",
      "           583, 29889],\n",
      "        [  315,  3235,  7228,  8569,   267,   373,  2472,  6993,  5925,   322,\n",
      "           338,  2729,   297,   317,  4025,  1182, 23075, 29892,   317,  4025,\n",
      "          1049, 29889]])\n",
      "Attention Masks: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Maximum Sequence Length after Padding: 22\n",
      "Tokens:\n",
      "['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '▁Germany', '▁is', '▁located', '▁in', '▁the', '▁middle', '▁of', '▁Europe', '.']\n",
      "['<|endoftext|>', '▁S', 'aar', 'land', '▁shares', '▁a', '▁border', '▁with', '▁France', '▁and', '▁is', '▁known', '▁for', '▁its', '▁strong', '▁Franco', '-', 'G', 'erman', '▁t', 'ies', '.']\n",
      "['▁C', 'IS', 'PA', '▁focus', 'es', '▁on', '▁information', '▁security', '▁research', '▁and', '▁is', '▁based', '▁in', '▁S', 'aar', 'br', 'ücken', ',', '▁S', 'aar', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "phi_batch = phi_tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(\"Phi Tokenizer Batch Outputs:\")\n",
    "print(\"Token IDs:\", phi_batch['input_ids'])\n",
    "print(\"Attention Masks:\", phi_batch['attention_mask'])\n",
    "phi_max_length = phi_batch['input_ids'].shape[1]\n",
    "print(\"Maximum Sequence Length after Padding:\", phi_max_length)\n",
    "\n",
    "phi_tokens_batch = [phi_tokenizer.convert_ids_to_tokens(ids) for ids in phi_batch['input_ids']]\n",
    "print(\"Tokens:\")\n",
    "for tokens in phi_tokens_batch:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemma Tokenizer Batch Outputs:\n",
      "Token IDs: tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      2,  30988,    603,   7023,    575,    573,   7185,    576,\n",
      "           4238, 235265],\n",
      "        [     2,   7595,    486,   1445,  14324,    476,   9994,    675,   6081,\n",
      "            578,    603,   3836,    604,   1277,   3779,  35961, 235290,  36419,\n",
      "          23572, 235265],\n",
      "        [     0,      0,      2,  93336,   4840,  31381,    611,   2113,   6206,\n",
      "           3679,    578,    603,   3482,    575,  96808, 164937, 235269,  96808,\n",
      "           1445, 235265]])\n",
      "Attention Masks: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Maximum Sequence Length after Padding: 20\n",
      "Tokens:\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<bos>', 'Germany', '▁is', '▁located', '▁in', '▁the', '▁middle', '▁of', '▁Europe', '.']\n",
      "['<bos>', 'Sa', 'ar', 'land', '▁shares', '▁a', '▁border', '▁with', '▁France', '▁and', '▁is', '▁known', '▁for', '▁its', '▁strong', '▁Franco', '-', 'German', '▁ties', '.']\n",
      "['<pad>', '<pad>', '<bos>', 'CIS', 'PA', '▁focuses', '▁on', '▁information', '▁security', '▁research', '▁and', '▁is', '▁based', '▁in', '▁Saar', 'brücken', ',', '▁Saar', 'land', '.']\n"
     ]
    }
   ],
   "source": [
    "gemma_batch = gemma_tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(\"\\nGemma Tokenizer Batch Outputs:\")\n",
    "print(\"Token IDs:\", gemma_batch['input_ids'])\n",
    "print(\"Attention Masks:\", gemma_batch['attention_mask'])\n",
    "gemma_max_length = gemma_batch['input_ids'].shape[1]\n",
    "print(\"Maximum Sequence Length after Padding:\", gemma_max_length)\n",
    "\n",
    "gemma_tokens_batch = [gemma_tokenizer.convert_ids_to_tokens(ids) for ids in gemma_batch['input_ids']]\n",
    "print(\"Tokens:\")\n",
    "for tokens in gemma_tokens_batch:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Decode the token IDs back into the sentences. Decode each sentence as well as the padded batch of sentences. Do not decode the padding tokens. Print the decoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Sentences (Phi Tokenizer):\n",
      "Germany is located in the middle of Europe.\n",
      "Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Sentences (Phi Tokenizer):\")\n",
    "for input_ids in phi_tokens:\n",
    "    decoded_sentence = phi_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Batch (Phi Tokenizer):\n",
      "Germany is located in the middle of Europe.\n",
      "Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n"
     ]
    }
   ],
   "source": [
    "decoded_batch = phi_tokenizer.batch_decode(phi_batch['input_ids'], skip_special_tokens=True)\n",
    "print(\"Decoded Batch (Phi Tokenizer):\")\n",
    "for sentence in decoded_batch:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Sentences (Gemma Tokenizer):\n",
      "Germany is located in the middle of Europe.\n",
      "Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded Sentences (Gemma Tokenizer):\")\n",
    "for input_ids in gemma_tokens:\n",
    "    decoded_sentence = gemma_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Batch (Gemma Tokenizer):\n",
      "Germany is located in the middle of Europe.\n",
      "Saarland shares a border with France and is known for its strong Franco-German ties.\n",
      "CISPA focuses on information security research and is based in Saarbrücken, Saarland.\n"
     ]
    }
   ],
   "source": [
    "decoded_batch = gemma_tokenizer.batch_decode(gemma_batch['input_ids'], skip_special_tokens=True)\n",
    "print(\"Decoded Batch (Gemma Tokenizer):\")\n",
    "for sentence in decoded_batch:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the models using the ```from_pretrained()``` method on the GPU. Print the model config, total number of parameters, amount of GPU memory utilized, and the dtype of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12058556bc341b5bca7d40aebfcc08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Model Config:\n",
      "Phi3Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"microsoft/Phi-3.5-mini-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0800000429153442,\n",
      "      1.1100000143051147,\n",
      "      1.1399999856948853,\n",
      "      1.340000033378601,\n",
      "      1.5899999141693115,\n",
      "      1.600000023841858,\n",
      "      1.6200000047683716,\n",
      "      2.620000123977661,\n",
      "      3.2300000190734863,\n",
      "      3.2300000190734863,\n",
      "      4.789999961853027,\n",
      "      7.400000095367432,\n",
      "      7.700000286102295,\n",
      "      9.09000015258789,\n",
      "      12.199999809265137,\n",
      "      17.670000076293945,\n",
      "      24.46000099182129,\n",
      "      28.57000160217285,\n",
      "      30.420001983642578,\n",
      "      30.840002059936523,\n",
      "      32.590003967285156,\n",
      "      32.93000411987305,\n",
      "      42.320003509521484,\n",
      "      44.96000289916992,\n",
      "      50.340003967285156,\n",
      "      50.45000457763672,\n",
      "      57.55000305175781,\n",
      "      57.93000411987305,\n",
      "      58.21000289916992,\n",
      "      60.1400032043457,\n",
      "      62.61000442504883,\n",
      "      62.62000274658203,\n",
      "      62.71000289916992,\n",
      "      63.1400032043457,\n",
      "      63.1400032043457,\n",
      "      63.77000427246094,\n",
      "      63.93000411987305,\n",
      "      63.96000289916992,\n",
      "      63.970001220703125,\n",
      "      64.02999877929688,\n",
      "      64.06999969482422,\n",
      "      64.08000183105469,\n",
      "      64.12000274658203,\n",
      "      64.41000366210938,\n",
      "      64.4800033569336,\n",
      "      64.51000213623047,\n",
      "      64.52999877929688,\n",
      "      64.83999633789062\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.0,\n",
      "      1.0199999809265137,\n",
      "      1.0299999713897705,\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0699999332427979,\n",
      "      1.0999999046325684,\n",
      "      1.1099998950958252,\n",
      "      1.1599998474121094,\n",
      "      1.1599998474121094,\n",
      "      1.1699998378753662,\n",
      "      1.2899998426437378,\n",
      "      1.339999794960022,\n",
      "      1.679999828338623,\n",
      "      1.7899998426437378,\n",
      "      1.8199998140335083,\n",
      "      1.8499997854232788,\n",
      "      1.8799997568130493,\n",
      "      1.9099997282028198,\n",
      "      1.9399996995925903,\n",
      "      1.9899996519088745,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0799996852874756,\n",
      "      2.0899996757507324,\n",
      "      2.189999580383301,\n",
      "      2.2199995517730713,\n",
      "      2.5899994373321533,\n",
      "      2.729999542236328,\n",
      "      2.749999523162842,\n",
      "      2.8399994373321533\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "\n",
      "Total Parameters in Phi Model: 3821079552\n",
      "GPU Memory Utilized (Phi Model): 14.23 GB\n",
      "Parameter Data Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "initial_memory_gpu = torch.mps.current_allocated_memory()\n",
    "phi_model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3.5-mini-instruct').to(device)\n",
    "final_memory_gpu = torch.mps.current_allocated_memory()\n",
    "print(\"Phi Model Config:\")\n",
    "print(phi_model.config)\n",
    "total_params = sum(p.numel() for p in phi_model.parameters())\n",
    "print(\"\\nTotal Parameters in Phi Model:\", total_params)\n",
    "gpu_memory = (final_memory_gpu - initial_memory_gpu) / (1024 ** 3)\n",
    "print(\"GPU Memory Utilized (Phi Model): {:.2f} GB\".format(gpu_memory))\n",
    "print(\"Parameter Data Type:\", next(phi_model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae3d3886372425a93ab43202cd0c323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff8b14b47844613b8f7a8fe0b0f6f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6559df827e804e23b76ed541d7691f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef34e4049fd84cdcbb03d74276daf087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24627735ab243ff9819e325ee9f618f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cea4579514499fac00aaad8ff3895c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19df83e174f4c02a193f1b673f5e19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Model Config:\n",
      "Gemma2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"google/gemma-2-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "\n",
      "Total Parameters in Gemma Model: 2614341888\n",
      "GPU Memory Utilized (Gemma Model): 9.74 GB\n",
      "Parameter Data Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "initial_memory_gpu = torch.mps.current_allocated_memory()\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b-it').to(device)\n",
    "final_memory_gpu = torch.mps.current_allocated_memory()\n",
    "print(\"Gemma Model Config:\")\n",
    "print(gemma_model.config)\n",
    "total_params = sum(p.numel() for p in gemma_model.parameters())\n",
    "print(\"\\nTotal Parameters in Gemma Model:\", total_params)\n",
    "gpu_memory = (final_memory_gpu - initial_memory_gpu) / (1024 ** 3)\n",
    "print(\"GPU Memory Utilized (Gemma Model): {:.2f} GB\".format(gpu_memory))\n",
    "print(\"Parameter Data Type:\", next(gemma_model.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate output with the instruction: <i>Explain the concept of large language models in simple terms for a beginner.</i> using the ```generate()``` method of each model. Generate a maximum of 256 new tokens. Print the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Explain the concept of large language models in simple terms for a beginner.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Model Response:\n",
      "Explain the concept of large language models in simple terms for a beginner.\n",
      "\n",
      "\n",
      "### Answer \n",
      "Large language models (LLMs) are advanced computer systems that can understand, generate, and respond to human language in a way that feels natural and conversational. Imagine a super-smart assistant that can read a book, write an essay, or even chat with you about your day. These models are trained on vast amounts of text data, which helps them learn the patterns and nuances of language.\n",
      "\n",
      "\n",
      "Here's how they work in simple terms:\n",
      "\n",
      "\n",
      "1. **Training Phase**: The model is fed a huge collection of text from books, websites, and other sources. It learns from this data by recognizing patterns, such as grammar rules, vocabulary, and common phrases.\n",
      "\n",
      "\n",
      "2. **Understanding**: When you type a question or a sentence, the model uses what it has learned to figure out what you're asking or saying. It doesn't understand language the way we do, but it can predict what comes next based on the patterns it has seen.\n",
      "\n",
      "\n",
      "3. **Generating Responses**: Based on its understanding, the model generates a response. It might write a summary,\n"
     ]
    }
   ],
   "source": [
    "inputs = phi_tokenizer(\n",
    "    instruction,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ").to(device)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "output_ids = phi_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    num_return_sequences=1,\n",
    "    # no_repeat_ngram_size=2,\n",
    ")\n",
    "\n",
    "output_text = phi_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Phi Model Response:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in the generated response: 256\n"
     ]
    }
   ],
   "source": [
    "output_tokens = phi_tokenizer.encode(output_text, add_special_tokens=False)\n",
    "# Number of generated tokens = total tokens of output - total tokens of input\n",
    "print(f\"Token count in the generated response: {len(output_tokens)-len(input_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Model Response:\n",
      "\n",
      "Explain the concept of large language models in simple terms for a beginner.\n",
      "\n",
      "Imagine you have a super-smart parrot that has read every book in the world. This parrot can understand your questions, follow your instructions, and even write stories, poems, and articles. That's kind of what a large language model (LLM) is!\n",
      "\n",
      "**Here's the breakdown:**\n",
      "\n",
      "* **Large:** These models are trained on massive amounts of text data, like books, articles, websites, and code.\n",
      "* **Language:** They are designed to understand and generate human language.\n",
      "* **Model:**  It's a complex mathematical representation of language that allows the model to learn patterns and relationships in words and sentences.\n",
      "\n",
      "**What can LLMs do?**\n",
      "\n",
      "* **Answer your questions:**  Think of it like having a super-smart encyclopedia at your fingertips.\n",
      "* **Write different kinds of creative content:**  From poems and stories to emails and articles, LLMs can help you express yourself.\n",
      "* **Translate languages:**  LLMs can help you understand text in different languages.\n",
      "* **Summarize text:**  LLMs can condense long pieces of writing into shorter summaries.\n",
      "* **Code:**  LLMs can even help you write code in different programming languages.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = gemma_tokenizer(\n",
    "    instruction,\n",
    "    return_tensors='pt',\n",
    ").to(device)\n",
    "\n",
    "output_ids = gemma_model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "output_text = gemma_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Gemma Model Response:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count in the generated response: 255\n"
     ]
    }
   ],
   "source": [
    "output_tokens = gemma_tokenizer.encode(output_text, add_special_tokens=False)\n",
    "# Number of generated tokens = total tokens of output - total tokens of input\n",
    "print(f\"Token count in the generated response: {len(output_tokens)-len(input_ids[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate 3 outputs for each model with the instruction: <i>Write a short story.</i> using top p (nucleus) sampling. Use different combinations of the top_p, top_k, and temperature parameters. Generate a maximum of 256 new tokens. Print the decoding parameters and the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write a short story.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"top_p\": 0.9, \"top_k\": 50, \"temperature\": 1.0},\n",
    "    {\"top_p\": 0.8, \"top_k\": 30, \"temperature\": 0.7},\n",
    "    {\"top_p\": 0.95, \"top_k\": 100, \"temperature\": 0.4},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi Model Responses:\n",
      "\n",
      "Decoding Parameters 1: {'top_p': 0.9, 'top_k': 50, 'temperature': 1.0}\n",
      "Generated Output 1:\n",
      "Write a short story.\n",
      "Create a suspenseful short story about a character named Alex who discovers a mysterious ancient map in their attic, leading to a hidden treasure in their backyard. Ensure that the story is structured with a clear beginning, middle, and end. Use the past tense consistently and include at least three direct dialogues. Limit the story to 500 words.\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Alex had always thought the attic was a mausoleum of forgotten memories. Dust-laden boxes and cobwebbed corners had been neglected for years. On a gloomy Saturday, as the rain played its monotonous symphony against the windowpane, Alex climbed the creaky steps, compelled by curiosity.\n",
      "\n",
      "The air was thick with nostalgia when Alex stumbled upon a brittle parchment, half-buried beneath an old, discarded tapestry. A map. The inked lines traced the outline of his very backyard, a treasure hunt crafted by time.\n",
      "\n",
      "\"Is it real?\" Alex whispered to the shadows.\n",
      "\n",
      "The following morning, Alex's excitement was palp\n",
      "Decoding Parameters 2: {'top_p': 0.8, 'top_k': 30, 'temperature': 0.7}\n",
      "Generated Output 2:\n",
      "Write a short story.\n",
      "A young girl discovers a magical garden in her backyard.\n",
      "\n",
      "In the heart of a bustling suburban neighborhood, nestled between the monotony of neatly trimmed lawns and cookie-cutter homes, there lay a secret that was about to unfold. Lily, a curious and imaginative seven-year-old, had always felt an inexplicable connection to her backyard. It was her sanctuary, a place where she could escape the demands of her structured world and let her imagination run wild.\n",
      "\n",
      "One sunny afternoon, as she dug through the soft soil, her fingers brushed against something unexpected. It was a small, ornate key, half-buried and gleaming with an otherworldly luster. Lily's heart raced with excitement. Could this be the key to something magical?\n",
      "\n",
      "Without hesitation, she began to search for a lock that matched the key. Hours turned into days as she explored every nook and cranny of her backyard. And then, one evening, as the sun dipped below the horizon, she stumbled upon it.\n",
      "\n",
      "Beneath the ancient oak tree\n",
      "Decoding Parameters 3: {'top_p': 0.95, 'top_k': 100, 'temperature': 0.4}\n",
      "Generated Output 3:\n",
      "Write a short story.\n",
      "Write a short story about a lost kitten finding its way home.\n",
      "\n",
      "### Answer:\n",
      "In the heart of a bustling city, a tiny calico kitten named Whiskers found herself lost amidst towering skyscrapers and noisy streets. Her curious nature had led her away from the warmth of her home, and now she stood in a crowded park, surrounded by strangers and unfamiliar sights.\n",
      "\n",
      "Whiskers meowed softly, her green eyes wide with fear. She missed the comforting scent of her mother and the familiar sound of her siblings' purrs. Just as she was about to give up hope, a gentle voice called out to her.\n",
      "\n",
      "\"Lost kitten, aren't you?\" said a kind-faced woman, holding out a hand.\n",
      "\n",
      "Whiskers hesitated for a moment, then cautiously approached the stranger. The woman smiled warmly and scooped her up into her arms, promising to help her find her way back home.\n",
      "\n",
      "As they walked through the city, the woman asked Whiskers about her home and the people she missed. Whiskers shared her memories,\n"
     ]
    }
   ],
   "source": [
    "print(\"Phi Model Responses:\")\n",
    "print()\n",
    "\n",
    "for i, params in enumerate(parameters):\n",
    "    print(f\"Decoding Parameters {i+1}: {params}\")\n",
    "    inputs = phi_tokenizer(\n",
    "        instruction,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    output_ids = phi_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        num_return_sequences=1,\n",
    "        top_p=params['top_p'],\n",
    "        top_k=params['top_k'],\n",
    "        temperature=params['temperature'],\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    output_text = phi_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Output {i+1}:\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Model Responses:\n",
      "\n",
      "Decoding Parameters 1: {'top_p': 0.9, 'top_k': 50, 'temperature': 1.0}\n",
      "Generated Output 1:\n",
      "Write a short story.\n",
      "\n",
      "The old lighthouse keeper, Elias, squinted at the swirling mist, his gnarled hand resting on the heavy brass telescope. The air, thick with the smell of brine and seaweed, carried a faint whisper of fear. For years, he’d seen storms roll in, their fury echoing the turmoil in his own heart. But this one felt different. This storm, it carried a cold, unnatural edge, whispering warnings he couldn't decipher.\n",
      "\n",
      "His old bones ached with the effort of cranking the lighthouse’s massive lamp, its light slicing through the oppressive darkness.  Each flicker, each pulse of light, was a battle won against the encroaching darkness. He had to ensure the ships returning from the open seas found safe passage home. But tonight, the fog clung tighter, obscuring the horizon like a shroud.\n",
      "\n",
      "Suddenly, a shape emerged from the swirling white abyss.  Elias felt a tremor run through his old bones.  It was not a ship.  It was a leviathan, its body as black as the abyss, eyes burning like molten gold.  It towered above the lighthouse, its tentacles reaching towards the sky.  The lighthouse groaned, strained by the sheer terror of the creature.\n",
      "\n",
      "Elias, a man who had\n",
      "Decoding Parameters 2: {'top_p': 0.8, 'top_k': 30, 'temperature': 0.7}\n",
      "Generated Output 2:\n",
      "Write a short story.\n",
      "\n",
      "The old lighthouse keeper, Silas, sat on his creaky porch, the salt spray stinging his weathered face. He squinted at the horizon, where the sea churned and frothed, a wild beast against the grey sky. The storm had been raging for days, a relentless assault on the rocky coast. He'd seen many storms in his seventy years, but this one felt different.\n",
      "\n",
      "Silas reached for his pipe, the cool metal a familiar comfort against his calloused fingers. He took a long drag, the smoke swirling around his head, and let out a sigh. He missed the days when the sea was calm, when he could hear the gentle lapping of waves against the rocks, and the rhythmic creaking of the lighthouse. Now, the only sound was the howling wind and the crashing waves.\n",
      "\n",
      "He remembered his wife, Amelia, her laughter like the chiming of a bell. She used to sit beside him on the porch, her hand in his, watching the sea. They'd talk about their dreams, their hopes, their fears. He'd tell her stories of the sea, of the creatures that lived in its depths, and she'd listen with a smile.\n",
      "\n",
      "But Amelia was gone now, taken by a sudden\n",
      "Decoding Parameters 3: {'top_p': 0.95, 'top_k': 100, 'temperature': 0.4}\n",
      "Generated Output 3:\n",
      "Write a short story.\n",
      "\n",
      "The rain hammered against the attic window, a relentless rhythm that mirrored the pounding in Amelia's chest. She clutched the dusty photograph, its edges softened by years of neglect. A faded image of a young woman with eyes that seemed to hold the very essence of the storm, her smile a promise of sunshine. \n",
      "\n",
      "Amelia's grandmother, the woman with the storm-filled eyes, had passed away just a week ago. Now, amidst the musty scent of forgotten memories and cobwebs, she found herself grappling with a grief that felt both familiar and alien. \n",
      "\n",
      "The attic was a labyrinth of forgotten dreams, a testament to a life lived and lost. Amelia's grandmother had been a whirlwind of energy, a woman who danced through life with an infectious joy. But beneath the surface, Amelia had always sensed a hidden sadness, a yearning for something she couldn't quite grasp. \n",
      "\n",
      "The photograph was the only tangible link to this elusive past. It was a snapshot of a time before the storm clouds had settled over her grandmother's eyes, a time when her smile seemed to light up the entire world. \n",
      "\n",
      "As the rain subsided, a faint glimmer of sunlight pierced through the attic window, casting long shadows on the dusty floor. Amelia felt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gemma Model Responses:\")\n",
    "print()\n",
    "\n",
    "for i, params in enumerate(parameters):\n",
    "    print(f\"Decoding Parameters {i+1}: {params}\")\n",
    "    input_ids = gemma_tokenizer(\n",
    "        instruction,\n",
    "        return_tensors='pt',\n",
    "    ).to(device)\n",
    "\n",
    "    output_ids = gemma_model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=256,\n",
    "        top_p=params['top_p'],\n",
    "        top_k=params['top_k'],\n",
    "        temperature=params['temperature'],\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    output_text = gemma_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Output {i+1}:\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load the quantized versions of each model using BitsAndBytesConfig. Load the 8 Bit and 4 Bit versions with default parameters. Print the model config, total number of parameters, and the amount of GPU memory utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```BitsAndBytes is not supported for mps backend yet. Running this on colab using cuda```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_8bit_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "bnb_4bit_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Occupied by Phi 8-bit Model: 3.78 GB\n",
      "\n",
      "Total Parameters in Phi 8-bit Model: 3821079552\n",
      "\n",
      "Phi Model 8-bit Quantized Config:\n",
      "Phi3Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"microsoft/Phi-3.5-mini-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0800000429153442,\n",
      "      1.1100000143051147,\n",
      "      1.1399999856948853,\n",
      "      1.340000033378601,\n",
      "      1.5899999141693115,\n",
      "      1.600000023841858,\n",
      "      1.6200000047683716,\n",
      "      2.620000123977661,\n",
      "      3.2300000190734863,\n",
      "      3.2300000190734863,\n",
      "      4.789999961853027,\n",
      "      7.400000095367432,\n",
      "      7.700000286102295,\n",
      "      9.09000015258789,\n",
      "      12.199999809265137,\n",
      "      17.670000076293945,\n",
      "      24.46000099182129,\n",
      "      28.57000160217285,\n",
      "      30.420001983642578,\n",
      "      30.840002059936523,\n",
      "      32.590003967285156,\n",
      "      32.93000411987305,\n",
      "      42.320003509521484,\n",
      "      44.96000289916992,\n",
      "      50.340003967285156,\n",
      "      50.45000457763672,\n",
      "      57.55000305175781,\n",
      "      57.93000411987305,\n",
      "      58.21000289916992,\n",
      "      60.1400032043457,\n",
      "      62.61000442504883,\n",
      "      62.62000274658203,\n",
      "      62.71000289916992,\n",
      "      63.1400032043457,\n",
      "      63.1400032043457,\n",
      "      63.77000427246094,\n",
      "      63.93000411987305,\n",
      "      63.96000289916992,\n",
      "      63.970001220703125,\n",
      "      64.02999877929688,\n",
      "      64.06999969482422,\n",
      "      64.08000183105469,\n",
      "      64.12000274658203,\n",
      "      64.41000366210938,\n",
      "      64.4800033569336,\n",
      "      64.51000213623047,\n",
      "      64.52999877929688,\n",
      "      64.83999633789062\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.0,\n",
      "      1.0199999809265137,\n",
      "      1.0299999713897705,\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0699999332427979,\n",
      "      1.0999999046325684,\n",
      "      1.1099998950958252,\n",
      "      1.1599998474121094,\n",
      "      1.1599998474121094,\n",
      "      1.1699998378753662,\n",
      "      1.2899998426437378,\n",
      "      1.339999794960022,\n",
      "      1.679999828338623,\n",
      "      1.7899998426437378,\n",
      "      1.8199998140335083,\n",
      "      1.8499997854232788,\n",
      "      1.8799997568130493,\n",
      "      1.9099997282028198,\n",
      "      1.9399996995925903,\n",
      "      1.9899996519088745,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0799996852874756,\n",
      "      2.0899996757507324,\n",
      "      2.189999580383301,\n",
      "      2.2199995517730713,\n",
      "      2.5899994373321533,\n",
      "      2.729999542236328,\n",
      "      2.749999523162842,\n",
      "      2.8399994373321533\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "phi_model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3.5-mini-instruct',\n",
    "    quantization_config=bnb_8bit_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "memory_utilized = (final_memory - init_memory) / (1024 ** 3)\n",
    "print(\"Memory Occupied by Phi 8-bit Model: {:.2f} GB\".format(memory_utilized))\n",
    "print()\n",
    "total_params_8bit = sum(p.numel() for p in phi_model_8bit.parameters())\n",
    "print(\"Total Parameters in Phi 8-bit Model:\", total_params_8bit)\n",
    "print()\n",
    "print(\"Phi Model 8-bit Quantized Config:\")\n",
    "print(phi_model_8bit.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Occupied by Phi 4-bit Model: 2.27 GB\n",
      "\n",
      "Total Parameters in Phi 4-bit Model: 2009140224\n",
      "\n",
      "Phi Model 4-bit Quantized Config:\n",
      "Phi3Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"microsoft/Phi-3.5-mini-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3.5-mini-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3.5-mini-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0800000429153442,\n",
      "      1.1100000143051147,\n",
      "      1.1399999856948853,\n",
      "      1.340000033378601,\n",
      "      1.5899999141693115,\n",
      "      1.600000023841858,\n",
      "      1.6200000047683716,\n",
      "      2.620000123977661,\n",
      "      3.2300000190734863,\n",
      "      3.2300000190734863,\n",
      "      4.789999961853027,\n",
      "      7.400000095367432,\n",
      "      7.700000286102295,\n",
      "      9.09000015258789,\n",
      "      12.199999809265137,\n",
      "      17.670000076293945,\n",
      "      24.46000099182129,\n",
      "      28.57000160217285,\n",
      "      30.420001983642578,\n",
      "      30.840002059936523,\n",
      "      32.590003967285156,\n",
      "      32.93000411987305,\n",
      "      42.320003509521484,\n",
      "      44.96000289916992,\n",
      "      50.340003967285156,\n",
      "      50.45000457763672,\n",
      "      57.55000305175781,\n",
      "      57.93000411987305,\n",
      "      58.21000289916992,\n",
      "      60.1400032043457,\n",
      "      62.61000442504883,\n",
      "      62.62000274658203,\n",
      "      62.71000289916992,\n",
      "      63.1400032043457,\n",
      "      63.1400032043457,\n",
      "      63.77000427246094,\n",
      "      63.93000411987305,\n",
      "      63.96000289916992,\n",
      "      63.970001220703125,\n",
      "      64.02999877929688,\n",
      "      64.06999969482422,\n",
      "      64.08000183105469,\n",
      "      64.12000274658203,\n",
      "      64.41000366210938,\n",
      "      64.4800033569336,\n",
      "      64.51000213623047,\n",
      "      64.52999877929688,\n",
      "      64.83999633789062\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.0,\n",
      "      1.0199999809265137,\n",
      "      1.0299999713897705,\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0699999332427979,\n",
      "      1.0999999046325684,\n",
      "      1.1099998950958252,\n",
      "      1.1599998474121094,\n",
      "      1.1599998474121094,\n",
      "      1.1699998378753662,\n",
      "      1.2899998426437378,\n",
      "      1.339999794960022,\n",
      "      1.679999828338623,\n",
      "      1.7899998426437378,\n",
      "      1.8199998140335083,\n",
      "      1.8499997854232788,\n",
      "      1.8799997568130493,\n",
      "      1.9099997282028198,\n",
      "      1.9399996995925903,\n",
      "      1.9899996519088745,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0199997425079346,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0299997329711914,\n",
      "      2.0799996852874756,\n",
      "      2.0899996757507324,\n",
      "      2.189999580383301,\n",
      "      2.2199995517730713,\n",
      "      2.5899994373321533,\n",
      "      2.729999542236328,\n",
      "      2.749999523162842,\n",
      "      2.8399994373321533\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "phi_model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/Phi-3.5-mini-instruct',\n",
    "    quantization_config=bnb_4bit_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "memory_utilized = (final_memory - init_memory) / (1024 ** 3)\n",
    "print(\"Memory Occupied by Phi 4-bit Model: {:.2f} GB\".format(memory_utilized))\n",
    "print()\n",
    "total_params_4bit = sum(p.numel() for p in phi_model_4bit.parameters())\n",
    "print(\"Total Parameters in Phi 4-bit Model:\", total_params_4bit)\n",
    "print()\n",
    "print(\"Phi Model 4-bit Quantized Config:\")\n",
    "print(phi_model_4bit.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Occupied by Gemma 8-bit Model: 2.99 GB\n",
      "\n",
      "Total Parameters in Gemma 8-bit Model: 2614341888\n",
      "\n",
      "Gemma Model 8-bit Quantized Config:\n",
      "Gemma2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"google/gemma-2-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "gemma_model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    'google/gemma-2-2b-it',\n",
    "    quantization_config=bnb_8bit_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "memory_utilized = (final_memory - init_memory) / (1024 ** 3)\n",
    "print(\"Memory Occupied by Gemma 8-bit Model: {:.2f} GB\".format(memory_utilized))\n",
    "print()\n",
    "total_params_8bit = sum(p.numel() for p in gemma_model_8bit.parameters())\n",
    "print(\"Total Parameters in Gemma 8-bit Model:\", total_params_8bit)\n",
    "print()\n",
    "print(\"Gemma Model 8-bit Quantized Config:\")\n",
    "print(gemma_model_8bit.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Occupied by Gemma 4-bit Model: 2.21 GB\n",
      "\n",
      "Total Parameters in Gemma 4-bit Model: 1602203904\n",
      "\n",
      "Gemma Model 4-bit Quantized Config:\n",
      "Gemma2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"google/gemma-2-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "gemma_model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    'google/gemma-2-2b-it',\n",
    "    quantization_config=bnb_4bit_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "memory_utilized = (final_memory - init_memory) / (1024 ** 3)\n",
    "print(\"Memory Occupied by Gemma 4-bit Model: {:.2f} GB\".format(memory_utilized))\n",
    "print()\n",
    "total_params_4bit = sum(p.numel() for p in gemma_model_4bit.parameters())\n",
    "print(\"Total Parameters in Gemma 4-bit Model:\", total_params_4bit)\n",
    "print()\n",
    "print(\"Gemma Model 4-bit Quantized Config:\")\n",
    "print(gemma_model_4bit.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
